{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce TP est entiérement basé sur les  TPs du cours de deep learning  de Andrew NG sur Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batir votre reseau de neurones profond: Pas à Pas\n",
    "\n",
    "Vous construirez un réseau de neurones profonds, avec autant de couches que vous le souhaitez!\n",
    "Dans ce notebook, vous implémenterez toutes les fonctions nécessaires à la création d’un réseau de neurones profonds.\n",
    "Vous pourrez utiliser ces fonctions pour créer un réseau de neurones profonds pour la classification des images! \n",
    "\n",
    "**Objectifs spécifiques**:\n",
    "- Utilisez des unités non linéaires comme ReLU pour améliorer votre modèle\n",
    "- Construire un réseau de neurones plus profond (avec plus d'une couche cachée)\n",
    "- Implémenter une classe de réseau neuronal facile à utiliser! \n",
    "\n",
    "**Notation**:\n",
    "- L'exposant $[l]$ represente une quantité associée à la  $l^{ieme}$ couche. \n",
    "    - Exemple: $a^{[L]}$ est la fonction d'activation de la  $L^{ieme}$ couche. $W^{[L]}$ et $b^{[L]}$ sont les paramétres de la  $L^{ieme}$ couche.\n",
    "-  L'exposant $(i)$ represente une quantité associée au $i^{ieme}$ exemple. \n",
    "    - Exemple: $x^{(i)}$ est le $i^{ieme}$ training exemple.\n",
    "- L'indice $i$ represente l'$i^{ieme}$ element d'un vecteur.\n",
    "    - Exemple: $a^{[l]}_i$ represente  l'$i^{ieme}$ element de la  $l^{ieme}$ couche d'activation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "\n",
    "Importation des packages dont vous aurez besoin pour cet exercice. \n",
    "- [numpy](www.numpy.org) package pour le calcul scientifique sous python.\n",
    "- [matplotlib](http://matplotlib.org) librairie pour créer des graphiques sous python.\n",
    "- dnn_utils fournit quelques fonctions utlitaires.\n",
    "- testCases fournit des cas de tests pour valider vos résultats\n",
    "- np.random.seed(1) est utilisé pour maintenir la cohérence de tous les appels de fonction aléatoires.  S'il vous plaît ne changez pas le seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v4 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # taille par defaut pour les graphiques\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Grandes lignes du TP\n",
    "\n",
    "Pour construire votre réseau de neurones, vous allez implémenter plusieurs fonctions utilitaires. Ces fonctions utilitaires seront utilisées pour construire un réseau de neurones à deux couches et un réseau de neurones à couche L. Chaque  fonction utilitaire que vous implémenterez aura des instructions détaillées qui vous guideront à travers les étapes nécessaires. Voici un aperçu du travail, vous allez:\n",
    "\n",
    "- Initialiser les parametres pour un reseau de neuronesà deux couches et pour un réseau de neurones à $L$ couches.\n",
    "- Implementer le module de forward propagation (en mauve dans la figure ci-dessous).\n",
    "     - Completer la partie LINEAIRE de chaque couche pour l'étape de forward propagation (donnera $Z^{[l]}$).\n",
    "     - Nous vous fournissons les fonctions d'ACTIVATION (relu/sigmoid) mais vous pouvez les écrire aussi.\n",
    "     - Combiner les deux précédentes étapes en une nouvelle forward fontion [LINEAR->ACTIVATION].\n",
    "     - Empilez L-1 fois les [LINEAR->RELU] forward fonction (pour les couches 1 à L-1) et ajouter à la fin une couche $L$ de type [LINEAR->SIGMOID]. Vous obtiendrez une nouvelle fonction  L_model_forward.\n",
    "- Calculer le loss fonction.\n",
    "- Implementer le module de backward propagation  (en rouge dans la figure ci-dessous).\n",
    "    - Completer la partie LINEAIRE de chaque couche pour l'étape backward propagation.\n",
    "    - Nous vous fournissons les gradients d'ACTIVATION (relu/sigmoid) mais vous pouvez les écrire aussi (relu_backward/sigmoid_backward) \n",
    "    - Combiner les deux précédentes étapes en une nouvelle backward fontion [LINEAR->ACTIVATION] .\n",
    "    - Empiler L-1 fois les [LINEAR->RELU] backward fonction et ajouter un backward [LINEAR->SIGMOID]  dans une nouvelle fonction L_model_backward\n",
    "- Mettre à jour les paramétres.\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> **Figure 1**</center></caption><br>\n",
    "\n",
    "\n",
    "**Note** Pour chaque fonction forward, il existe une fonction backward correspondante. C'est pourquoi, à chaque étape de votre FORWARD module, vous stockerez certaines valeurs dans un cache. Les valeurs mises en cache sont utiles pour le calcul des gradients. Dans le BACKWARD module, vous utiliserez ensuite le cache pour calculer les gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 - Initialisation\n",
    "\n",
    "Vous allez écrire deux helper fonctions qui initialiseront les paramètres de votre modèle. La première fonction sera utilisée pour initialiser les paramètres d'un modèle à deux couches. Le second généralisera ce processus d’initialisation à un modéle à $ L $ couches .\n",
    "\n",
    "### 3.1 - Reseau de neurones à deux couches\n",
    "\n",
    "**Exercise**: Créez et initialisez les paramètres du réseau de neurones à 2 couches.\n",
    "\n",
    "**Instructions**:\n",
    "- La structure du modele est: *LINEAR -> RELU -> LINEAR -> SIGMOID*. \n",
    "- Utiliser une génération alétoire des poids. Utiliser `np.random.randn(shape)*0.01` avec les bonnes dimensions.\n",
    "- initialiser les biais à zero. Use `np.zeros(shape)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- taille de la couche d'input\n",
    "    n_h -- taille de la couche cachée\n",
    "    n_y -- taille de la couche d'output\n",
    "    \n",
    "    Returns:\n",
    "    parametres -- dictionnaire python contenant les paramétres:\n",
    "                    W1 -- matrice des poids de dimensions (n_h, n_x)\n",
    "                    b1 -- vecteur des biais de dimension (n_h, 1)\n",
    "                    W2 -- matrice des poids de dimensions (n_y, n_h)\n",
    "                    b2 -- vecteur des biais de dimension (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### Initialiser les paramétres ### (≈ 4 lines of code)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### FIN  ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    ### Commpleter le dictionnaire ### (≈ 4 lines of code)\n",
    "    \n",
    "        \n",
    "    ### FIN ###\n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]] </td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[ 0.01744812 -0.00761207]]</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> **b2** </td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 - Reseau de neurones à L-couche\n",
    "\n",
    "L'initialisation d'un réseau de neurones à L couches est plus compliquée car il existe beaucoup plus de matrices de poids et de vecteurs de biais. Quand vous aurez complété `initialize_parameters_deep`, vous devez vous assurer que les dimmensions correspondent dans chaque couche. Souvenez vous que $n^{[l]}$ est le nombre d'unités dans la couche $l$. Par exemple si taille des input $X$ est $(12288, 209)$ (avec $m=209$ exemples) alors:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\n",
    "\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> **Dimensions de W** </td> \n",
    "        <td> **Dimensions de b**  </td> \n",
    "        <td> **Activation** </td>\n",
    "        <td> **Dimension Activation** </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Couche 1** </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Couche 2** </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "   \n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>\n",
    "    \n",
    "   <tr>\n",
    "        <td> **Couche L-1** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    \n",
    "   <tr>\n",
    "        <td> **Couche L** </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "**Broadcasting sous python**\n",
    "\n",
    "Le caclcul de $W X + b$ sous python est simple grace au broadcasting. Par exemple, si: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "Alors $WX + b$ sera:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implementer l'initialization d'un reseau de neurone à L-Couches. \n",
    "\n",
    "**Instructions**:\n",
    "- La structure du  modéle est *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. I.e., il a  $L-1$ couches utilisant  ReLU comme fonction d'activation suivi par un sigmoid comme fonction d'activation dans la couche output.\n",
    "- Initialiser aléatoirement la matrice des poids. Utiliser `np.random.randn(shape) * 0.01`.\n",
    "- Initialiser les biais avec des zeros. Utiliser `np.zeros(shape)`.\n",
    "- Vous stockerez $n^{[l]}$, le nombre d'unité dans les différentes couches, dans une variable `layer_dims`. Par exemple, Si `layer_dims` vaut [2,4,1] alors:  Il y a 2 inputs, une couche cachée avec  4  unités, et une couche output  avec 1 unite. Ceci signifie que les dimensions de `W1` sont (4,2), de `b1` sont (4,1),de `W2` sont (1,4) et de `b2` sont (1,1). Vous généraliserez cela à $L$ couches! \n",
    "- Dans l'exemple ci-dessous, une implémentation pour $L=1$ (Un réseau à une couche cachée). \n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) contenant les dimensions de chaque couche du reseau\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- dictionnaire python contenant les paramétres \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- matrice des poids de dimensions (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- vecteur des biais de la forme (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # nombre de couches dans le reseau\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Module de Forward propagation\n",
    "\n",
    "### 4.1 - Linear Forward \n",
    "\n",
    "Maintenant que vous avez initialisé vos paramètres, vous allez créer le module de FORWARD propagation. Vous allez commencer par implémenter certaines fonctions de base que vous utiliserez ultérieurement lors de l'implémentation du modèle. Vous allez implémenter trois fonctions dans cet ordre:\n",
    "\n",
    "- LINEAIRE\n",
    "- LINEAIRE -> ACTIVATION où ACTIVATION sera soit ReLU soit Sigmoid. \n",
    "- [LINEAIRE -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (modéle complet)\n",
    "\n",
    "Le module FORWARDlinéaire (vectorisé sur tous les exemples) calcule les équations suivantes:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "where $A^{[0]} = X$. \n",
    "\n",
    "**Exercise**: Implémenter la partie linéaire du module FORWARD.\n",
    "\n",
    "**Rappel**:\n",
    "La représentation mathématique de cet unité est $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$. la fonction `np.dot()` de numpy est trés utile. Si les dimensions ne correspondent pas, afficher `W.shape` peut aider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations de la couche précédente (or input): (nombre d'unités de la couche précédente, nombre d'exemples)\n",
    "    W -- matrice des poids: numpy array de dimensions (nombre d'unités de la couche courante, nombre d'unités de la couche précédente)\n",
    "    b -- vecteur des biais, numpy array de dimensions (nombre d'unités de la couche courante, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z --  input de la fonction d'activation, aussi appelé paramétre de pre-activation \n",
    "    cache -- un dictionnaire python qui contient \"A\", \"W\" and \"b\" ; \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Implementer la partie linéaire ### \n",
    "    \n",
    "    \n",
    "    ### FIN ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    ### Implementer le cache ### \n",
    "    \n",
    "    ### FIN ###\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Linear-Activation Forward\n",
    "\n",
    "Vous utiliserez deux fonctions d'activation:\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. Nous avons fourni la fonction `sigmoid`. La fonction retourne  **two** items: la valeur de l'activation \"`a`\" et un \"`cache`\" qui contient \"`Z`\" (sera fourni à la fonction BACKWARD). L'exemple ci-dessous permet de faire l'appel: \n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: La formulation mathématique du RELU est $A = RELU(Z) = max(0, Z)$. Nous avons fourni la fonction `relu`. La fonction retourne  **two** items: la valeur de l'activation \"`a`\" et un \"`cache`\" qui contient \"`Z`\" (sera fourni à la fonction BACKWARD). L'exemple ci-dessous permet de faire l'appel: \n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour plus de commodité, vous allez regrouper deux fonctions (Linéaire et Activation) en une seule fonction (LINEAR-> ACTIVATION). Par conséquent, vous allez implémenter une fonction qui effectue une étape  LINEAIRE suivie d’une étape  ACTIVATION.\n",
    "\n",
    "**Exercice**: Implementer la fonction forward propagation de la couche *LINEAR->ACTIVATION*. La relation mathématique est: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ où la fonction d'activation \"g\" peut être sigmoid() ou relu(). Utiliser linear_forward() et la bonne  fonction d'activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implementer la fonction forward propagation de la couche *LINEAR->ACTIVATION*\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations de la couche précédente (or input): (nombre d'unités de la couche précédente, nombre d'exemples)\n",
    "    W -- matrice des poids: numpy array de dimensions (nombre d'unités de la couche courante, nombre d'unités de la couche précédente)\n",
    "    b -- vecteur des biais, numpy array de dimensions (nombre d'unités de la couche courante, 1)\n",
    "    activation -- l'activation à utiliser dans la couche courante, stocké sous forme de texte: \"sigmoid\" ou \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- l'output de la fonction d'activation, appelée aussi valeur post-activation \n",
    "    cache -- un dictionnaire python contenant \"linear_cache\" et \"activation_cache\";\n",
    "             stocké pour calculer le backward pass de maniére efficiente.\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    ### IMPLEMENTER LE CACHE ### \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### FIN ### \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A ** </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A ** </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: En deep learning, le calcul \"[LINEAR-> ACTIVATION]\" est compté comme une seule couche dans le réseau neuronal, pas deux couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Modéle à L-couche \n",
    "\n",
    "Pour encore plus de commodité lors de l’implémentation du reseau à $ L $ couches, vous aurez besoin d’une fonction qui réplique $ L-1 $ fois la couche `linear_activation_forward` avec RELU , puis de la fonction` linear_activation_forward` avec SIGMOID.\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> **Figure 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>\n",
    "\n",
    "**Exercice**: Implementer le forward propagation du modéle ci-dessus.\n",
    "\n",
    "**Instruction**: Dans le code ci-dessous, la variable `AL` représente $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (Elle est appelée aussi `Yhat`, i.e., $\\hat{Y}$.) \n",
    "\n",
    "**Tips**:\n",
    "- Utiliser les fonctions écrites précédemment  \n",
    "- Utiliser une boucle  for pour repliquer (L-1) fois [LINEAR->RELU] \n",
    "- N'oubliez pas de garder une trace des caches dans la liste \"caches\". Pour ajouter une nouvelle valeur `c` à une` list`, vous pouvez utiliser `list.append (c)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implementer le forward propagation pour le calcul de [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array de dimension (input size, number d'exemples)\n",
    "    parameters -- output de initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- valeur de la derniére  post-activation\n",
    "    caches -- liste des caches contenant:\n",
    "                chaque cache of linear_activation_forward() (Il y en a  L-1, indexé  0 à L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number de couches dans le reseau\n",
    "    \n",
    "    # Implementer [LINEAR -> RELU]*(L-1). Ajouter \"cache\" à la liste des\"caches\".\n",
    "    \n",
    "    \n",
    "    \n",
    "    #########FIN##########\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### FIN ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Length of caches list ** </td>\n",
    "    <td > 3 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Génial! Vous avez maintenant une FORWARD propogation compléte  qui prend en entrée X et génère un vecteur ligne $A^{[L]}$ contenant vos prédictions. Il enregistre également toutes les valeurs intermédiaires dans des \"caches\". Utiliser $A^{[L]}$ pour calculer la fonction coût de vos prévisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Cost function\n",
    "\n",
    "\n",
    "Vous allez maintenant implémenter la FORWARD et BACKWARD propagation. Vous devez calculer le coût, car vous voulez vérifier si votre modèle est en train d'apprendre.\n",
    "\n",
    "**Exercise**: Calculer la fonction coût en utilisant la fonction cross-entropy  $J$, avec la formule suivante: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implementer la fonction coût définie dans l'equation  (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL --  vecteur de probabilité correspondant aux labels predits, dimensions (1, nombre d'exemples)\n",
    "    Y -- vecteur des vrais \"label\"  , dimensions (1, nombre d'exemples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost) # Pour être sûr que ta fonction coût correspond à ce qui est attendue  (e.g. change [[17]] en 17).\n",
    "    print(cost.shape)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "()\n",
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "\n",
    "    <tr>\n",
    "    <td>**cost** </td>\n",
    "    <td> 0.41493159961539694</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 -Module de Backward propagation\n",
    "\n",
    "\n",
    "Comme avec la FORWARD propagation, vous allez implémenter des helper fonctions  pour la BACKWARD propagation. Rappelez-vous que BACKWARD propagation est utilisée pour calculer le gradient de la loss fonction  en fonction des paramètres.\n",
    "\n",
    "**Rappel**: \n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> **Figure 3** : Forward et Backward propagation pour *LINEAR->RELU->LINEAR->SIGMOID* <br> *Les blocs mauves representent le forward propagation, et les blocs rouges representent le backward propagation.*  </center></caption>\n",
    "\n",
    "\n",
    "\n",
    "Maintenant, comme pour la FORWARD propagation, vous allez construire la BACKWARD propagation en trois étapes:\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward où ACTIVATION calcule soit la dérivée  ReLU ou celle  de sigmoid\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (tout le modéle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Linear backward\n",
    "\n",
    "Pour une couche $l$, la partie linéaire est: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (suivie d'une activation).\n",
    "\n",
    "Supposons que vous avez deja calculé la dérivée $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Vous voulez avoir $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figure 4** </center></caption>\n",
    "\n",
    "Les trois outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ sont calculés en utilisant l'input $dZ^{[l]}$.Voici les formules dont vous aurez besoin:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**: Utiliser les 3 formules ci-dessus pour implementer linear_backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implementer la portion linéaire du backward propagation pour une seule couche  (couche  l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient du cout en fonction de la partie linéaire de l' output de la couche courante l\n",
    "    cache -- tuple de valeurs (A_prev, W, b) provenant de la forward propagation de la couche courante\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient du cout en fonction de l'activation de la couche précédente  l-1,( même dimension que A_prev)\n",
    "    dW -- Gradient du cout en fonction de W (couche courante l), ( même dimension que W)\n",
    "    db -- Gradient du cout en fonction de b (couche courante l), ( même dimension que b)\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "   \n",
    "\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td> **dA_prev** </td>\n",
    "    <td > [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **dW** </td>\n",
    "        <td > [[-0.10076895  1.40685096  1.64992505]] </td> \n",
    "    </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **db** </td>\n",
    "        <td> [[ 0.50629448]] </td> \n",
    "    </tr> \n",
    "    \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear-Activation backward\n",
    "\n",
    "Ensuite, vous allez créer une fonction qui fusionne les deux helper fonctions: **`linear_backward`** et l'étape  backward pour l'activation **`linear_activation_backward`**. \n",
    "\n",
    "Pour vous aider à l'implementer `linear_activation_backward`, nous vous fournissons deux fonctions backward:\n",
    "- **`sigmoid_backward`**: Implemente le backward propagation de l'unité SIGMOID. Vous pouvez l'appeler comme suit: \n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**:  Implemente le backward propagation de l'unité SIGMOID. Vous pouvez l'appeler comme suit: \n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "Si $g(.)$ est la fonction d'activation, \n",
    "`sigmoid_backward` et `relu_backward` calculent $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    "\n",
    "**Exercise**: Implementer le backpropagation pour la couche *LINEAR->ACTIVATION* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implemente le backward propagation pour la couche LINEAR->ACTIVATION.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient pour la couche courante l \n",
    "    cache -- tuple de valeurs (linear_cache, activation_cache) stocké pour calculer backward propagation de maniére efficiente\n",
    "    activation -- l'activation sera utilisée dans la couche, stockée sous forme de texte: \"sigmoid\" ou \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient du cout en fonction de l'activation de la couche précédente  l-1,( même dimension que A_prev)\n",
    "    dW -- Gradient du cout en fonction de W (couche courante l), ( même dimension que W)\n",
    "    db -- Gradient du cout en fonction de b (couche courante l), ( même dimension que b)\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output with sigmoid:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td >[[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]] </td> \n",
    "\n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.05729622]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output with relu:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td> \n",
    "\n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.20837892]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L-Model Backward \n",
    "\n",
    "Now you will implement the backward function for the whole network. Recall that when you implemented the `L_model_forward` function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you will use those variables to compute the gradients. Therefore, in the `L_model_backward` function, you will iterate through all the hidden layers backward, starting from layer $L$. On each step, you will use the cached values for layer $l$ to backpropagate through layer $l$. Figure 5 below shows the backward pass.\n",
    "\n",
    "Vous allez maintenant implémenter la fonction BACKWARD pour tout le réseau. Rappelez-vous que lorsque vous avez implémenté la fonction L_model_forward, à chaque itération, vous avez stocké un cache qui contient (X, W, b et z). Dans le module de BACKWARD propagation, vous utiliserez ces variables pour calculer les gradients. Par conséquent, dans la fonction L_model_backward, vous parcourerez toutes les couches cachées en arrière, à partir de la couche\n",
    "L. À chaque étape, vous utiliserez les valeurs mises en cache pour la couche l pour faire une rétropropagation à travers la couche l. La figure 5 ci-dessous montre la passe en arrière.\n",
    "\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  **Figure 5** : Backward pass  </center></caption>\n",
    "\n",
    "** Initialiser backpropagation**:\n",
    "Pour faire de retropropagation dans ce reseau, nous savons que l'output est, \n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. Votre code doit alors calculer `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "Pour cela, utiliser la formule :\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "```\n",
    "\n",
    "Vous pouvez alors utiliser le gradient post-activation  `dAL` pour faire du backward. As seen in Figure 5, you can now feed in `dAL` into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function). After that, you will have to use a `for` loop to iterate through all the other layers using the LINEAR->RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula :\n",
    "\n",
    "Comme le montre la figure 5, vous pouvez maintenant alimenter `dAL` dans la fonction backward LINEAR-> SIGMOID que vous avez implémentée (qui utilisera les valeurs mises en cache stockées par la fonction L_model_forward). Après cela, vous devrez utiliser une boucle `for` pour parcourir toutes les autres couches en utilisant la fonction de retour LINEAR-> RELU. Vous devriez stocker chaque dA, dW et db dans le dictionnaire des gradients. Pour ce faire, utilisez cette formule:\n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "Par exemple, for $l=3$ devrait stocker $dW^{[l]}$ in `grads[\"dW3\"]`.\n",
    "\n",
    "**Exercise**: Implementer backpropagation for the *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implementer le backward propagation pour [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
    "    \n",
    "    Arguments:\n",
    "    AL -- vecteur de probabilité, output de la forward propagation (L_model_forward())\n",
    "    Y -- vecteur des vrais \"label\" \n",
    "    caches -- liste des caches contenant:\n",
    "                chaque cache de linear_activation_forward() avec \"relu\" (caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                la cache de linear_activation_forward() sigmoid \"sigmoid\" (caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- Un dictionnaire avec les gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td> \n",
    "  </tr> \n",
    "  \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]] </td> \n",
    "\n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Mettre à jour les  parametres\n",
    "\n",
    "Dans cette section, vous allez mettre à jour les paramètres du modèle, en utilisant la descente de gradient:\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "où $\\alpha$ est le learning rate. Après avoir calculé les paramètres mis à jour, stockez-les dans le dictionnaire de paramètres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implementer `update_parameters()` pour mettre à jour en utilisant la descente de gradient.\n",
    "\n",
    "**Instructions**:\n",
    "Mettre à jour les  parametres en utilisant la descente de gradient pour chaque $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "   Mettre à jour les  parametres en utilisant la descente de gradient\n",
    "    \n",
    "    Arguments:\n",
    "    parametres --  dictionaire python contenant vos parametres \n",
    "    grads -- dictionaire python contenant vos gradients, output de L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- dictionaire python contenant vos paramétres à jour \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    \n",
    "    \n",
    "    ### FIN ### (≈ 3 lines of code)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td > W1 </td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7 - Conclusion\n",
    "\n",
    "Félicitations pour avoir mis en place toutes les fonctions nécessaires à la construction d’un réseau de neurones profonds!\n",
    "Nous savons que la tâche a été longue, mais nous ne pouvons que nous améliorer. \n",
    "Vous réunirez tous ces éléments pour créer deux modèles:\n",
    "\n",
    "-Un réseau de neurones à deux couches\n",
    "\n",
    "-Un réseau de neurones de couche L\n",
    "\n",
    "Vous utiliserez en fait ces modèles pour faire de la classification binaire!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
